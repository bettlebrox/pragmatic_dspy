{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pragmatic DSPy\n",
    "\n",
    "This is the companion notebook to this [blog post](https://dass.ie/blog/pragmatic-dspy)\n",
    "\n",
    "**First** set the env variables in local.env before executing the first cell below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "dotenv.load_dotenv('local.env')\n",
    "import dspy\n",
    "import os\n",
    "from langfuse import Langfuse\n",
    "from langfuse.api.resources.commons.types import DatasetStatus\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import glob\n",
    "#DSPy is using litellm to call the LLM so its useful to tweak the litellm config\n",
    "import litellm\n",
    "from extractor import GroundTruthEvaluator\n",
    "from extractor import Extractor\n",
    "from extractor import Extractor_Signature\n",
    "from extractor import fetch_html\n",
    "import uuid\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configure\n",
    "\n",
    "- DSPy will use OpenAI 4o-mini by default.\n",
    "- Langfuse is used for tracing and logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "litellm.success_callback = [\"langfuse\"]\n",
    "litellm.failure_callback = [\"langfuse\"]\n",
    "#langfuse client is used for storing and retrieving datasets\n",
    "langfuse = Langfuse(secret_key=os.getenv('LANGFUSE_SECRET_KEY'),\n",
    "    public_key=os.getenv('LANGFUSE_PUBLIC_KEY'),\n",
    "    host=os.getenv('LANGFUSE_HOST'))\n",
    "\n",
    "#4o-mini was quick to get up and running\n",
    "lm = dspy.LM('openai/gpt-4o-mini', api_key=os.getenv('OPENAI_API_KEY'),temperature=0.0)\n",
    "dspy.configure(lm=lm,suppress_debug_info=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Bootstrap and review dataset\n",
    "\n",
    "Let's crawl some websites and see if we can get a sense of the quality of the data\n",
    "\n",
    "**Note**: always comply with the T&Cs of the websites you're scraping\n",
    "\n",
    "simple_extractor is a very simple DSPy Signature that expects html and url as input and returns some event info see extractor.py\n",
    "\n",
    "lets start with a trivial synthetic example to see if it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_extractor = dspy.Predict(Extractor_Signature)\n",
    "prediction = simple_extractor(html=\"\"\"\n",
    "<html>\n",
    "<body>\n",
    "<h1>Event Title</h1>\n",
    "<p>Event Description</p>\n",
    "<p>Event Location</p>\n",
    "<p>Start: 1pm 12th December</p>\n",
    "<p>End: 2pm 12th December</p>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\",url=\"https://bobsevents.com/event1\")\n",
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It worked!\n",
    "\n",
    "Now let's try something a little more realistic and pull an actual event page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url = \"<event_page_url>\" #replace with a real url\n",
    "html = fetch_html(url)\n",
    "prediction = simple_extractor(html=html, url=url)\n",
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It feels a little magical that we only had to write a very simple Signature and we got a pretty good result\n",
    "\n",
    "We can have a look at the raw LLM call if we want to see what was sent to the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.history[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the extractor appears to be working, at least for two examples . Lets get some more data from a variety of sources so we can start to evaluate it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crawl import crawl\n",
    "#Optional crawls the websites(subpages of the url) and saves the html to a file in data folder\n",
    "urls = [\"<url>\"]\n",
    "for url in urls:\n",
    "    crawl(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next lets run the extractor over all the data from the last step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(data_dir: str, extractor: Extractor):\n",
    "    run_id = str(uuid.uuid4())\n",
    "    print(f\"Running with run_id: {run_id}\")\n",
    "    json_files = glob.glob(os.path.join(data_dir, \"*.json\"))\n",
    "    for json_file in json_files:\n",
    "        with open(json_file, \"r+\") as f:\n",
    "            if f.readable() and f.read().strip(): \n",
    "                f.seek(0)\n",
    "                data = json.load(f)\n",
    "                soup = BeautifulSoup(data[\"html\"], 'html.parser')\n",
    "                body = soup.find('body')\n",
    "                for script in soup.find_all('script'):\n",
    "                    #remove scripts as they can contain a lot of noise\n",
    "                    script.decompose()\n",
    "                try:\n",
    "                    extractor.extract(html=body.decode_contents(), url=data[\"metadata\"][\"url\"],run_id=run_id)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error extracting {json_file}: {e}\")\n",
    "            else:\n",
    "                print(f\"file not readable:{json_file}\")\n",
    "    return run_id\n",
    "\n",
    "simple_extractor = Extractor()\n",
    "run(\"data/synthetic\",simple_extractor)\n",
    "#you may want to run for other websites\n",
    "#run(\"data/<amazingevents>\",simple_extractor)\n",
    "langfuse.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can easily do a quick check of the results in [langfuse](https://cloud.langfuse.com/) using the run_id tag to filter. \n",
    "\n",
    "We could do lm.history again to see the raw calls but I found once you are making a few LLM calls the langfuse interface is easier to use\n",
    "\n",
    "Much like bootstrapping a dataset in order to fine tune a model in ([Lesson 1 - fastai](https://www.kaggle.com/code/jhoward/is-it-a-bird-creating-a-model-from-your-own-data)) we can now create a dataset to optimise the extractor\n",
    "\n",
    "Let's bootstrap using the simple_extractor and evaluate from there. We'll store datasets in langfuse so we can easily load them later. Of course for the purposes of this tutorial we could just store the dataset in memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_traces_to_dataset(run_id: str, dataset_name: str):\n",
    "    langfuse.create_dataset(name=dataset_name,description=\"pragmatic dspy events\")\n",
    "    traces = langfuse.fetch_traces(tags=[run_id])\n",
    "    for trace in traces.data:\n",
    "        langfuse.create_dataset_item(dataset_name=dataset_name,input=json.loads(trace.input),expected_output=json.loads(trace.output),metadata={\"trace_run_id\":run_id},source_trace_id=trace.id)\n",
    "\n",
    "#add whatever runs for the early steps we are happy with to the dataset\n",
    "add_traces_to_dataset('<run_id>','events')\n",
    "#add_traces_to_dataset('<run_id>','events')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Review the data\n",
    "Now we can navigate to the dataset in [langfuse](https://cloud.langfuse.com/) and get a feel for the data. In my case, for the websites I crawled, the extractor performed well. The main issue upon manual inspection of the data is that the extractor often speculates on an end time when there is none specified. Another issue is that some of the pages are listing with multiple events.\n",
    "\n",
    "Now let's create a metric to simulate the manual evaluation of the data we've just done. Open up extractor.py to see the signature for the Evaluator\n",
    "\n",
    "It's another simple dspy.Predict that expects html, url, title, description, location, start_time and end_time as input, i.e. the inputs plus the outputs of the extractor, and returns a score and reasoning\n",
    "\n",
    "So we're using an LLM to evaluate the extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ground_truth_evaluator = GroundTruthEvaluator()\n",
    "def run_evaluation(name, model: str = \"openai/gpt-4o-mini\",dataset_name: str = \"events\", extractor: Extractor = Extractor(), evaluator: GroundTruthEvaluator = GroundTruthEvaluator(),set: str = None):\n",
    "    dataset = langfuse.get_dataset(dataset_name)\n",
    "    for item in dataset.items:\n",
    "        print(item.id)\n",
    "        if set is not None:\n",
    "            if item.metadata[\"set\"] != set:\n",
    "                continue\n",
    "        if item.status != DatasetStatus.ARCHIVED:\n",
    "            with item.observe(run_name=name, run_metadata={\"model\": model}) as trace_id:\n",
    "                try:\n",
    "                    pred = extractor.extract(html=item.input[\"kwargs\"][\"html\"], url=item.input[\"kwargs\"][\"url\"],model=model)\n",
    "                    eval = evaluator.evaluate(html=item.input[\"kwargs\"][\"html\"], url=item.input[\"kwargs\"][\"url\"], title=pred.title, description=pred.description, location=pred.location, start_time=pred.start_time, end_time=pred.end_time)\n",
    "                    langfuse.score(name=\"GroundTruthEvaluator\",value=eval.score,trace_id=trace_id,comment=eval.reasoning)\n",
    "                    print(eval.score,\" \",eval.reasoning)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error evaluating {item.id}: {e}\")\n",
    "\n",
    "\n",
    "run_evaluation(\"baseline\",dataset_name=\"events\")\n",
    "langfuse.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing to notice is that html pages, even just the contents of body, can have very large token size so we'll need to reduce the size of the pages we use for training. DSPy add its own instructions to the LLM when optimising so we'll risk creating requests that are too large for the LLM to handle.\n",
    "\n",
    "We can use another simple dspy.Predict, that expects html and url as input and returns a boolean indicating if the page is a singular event page and the relevant html as output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from extractor import singular_event_page_evaluator\n",
    "ds = langfuse.get_dataset(\"events\")\n",
    "ds_reduced = langfuse.create_dataset(name=\"events_reduced\",description=\"pragmatic dspy events reduced\")\n",
    "for item in ds.items:\n",
    "    pred = singular_event_page_evaluator(html=item.input[\"kwargs\"][\"html\"], url=item.input[\"kwargs\"][\"url\"])\n",
    "    if pred.is_singular:\n",
    "        input = item.input\n",
    "        input[\"kwargs\"][\"html\"] = pred.relevant_html\n",
    "        langfuse.create_dataset_item(dataset_name=ds_reduced.name,input=input,expected_output=item.expected_output,metadata=item.metadata)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we got a dataset with just in the inputs we expect, lets try to deal with the speculation of end time\n",
    "\n",
    "Note: you're dataset may have different quirks but the general approach will be the same\n",
    "\n",
    "Let's run a contrived example to show how the evaluator prefers a speculated end time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_evaluator = GroundTruthEvaluator()\n",
    "test_html = \"\"\"\n",
    "<div class=\"event-details\">\n",
    "    <h1>An Event</h1>\n",
    "    <p class=\"description\">Great event</p>\n",
    "    <div class=\"location\">Dublin</div>\n",
    "    <div class=\"datetime\">\n",
    "        <time datetime=\"2024-12-12T13:00:00\">1pm, 12th December 2024</time>\n",
    "    </div>\n",
    "</div>\n",
    "\"\"\"\n",
    "eval1 = ground_truth_evaluator.evaluate(html=test_html, url=\"https://bobsevents.com/event1\", title=\"An Event\", description=\"Great event\", location=\"Dublin\", start_time=\"2024-12-12T13:00:00\", end_time=None)\n",
    "eval2 = ground_truth_evaluator.evaluate(html=test_html, url=\"https://bobsevents.com/event1\", title=\"An Event\", description=\"Great event\", location=\"Dublin\", start_time=\"2024-12-12T13:00:00\", end_time=\"2024-12-12T15:00:00\")\n",
    "print(eval1)\n",
    "print(eval2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create an eval dataset and bias the evaluator to prefer leaving the end time as None if it is not specified in the html\n",
    "\n",
    "I've reviewed the dataset so I can automate biasing the evaluator. Of course we could do this manually in langfuse by manually labelling the dataset with our expected output.\n",
    "\n",
    "Its worth noting how lacking in rigour my metric is buts it's not hard imagine how we could add more rigorous metrics as we expand and refine the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ds = langfuse.get_dataset(\"events_reduced\")\n",
    "eval_dataset = langfuse.create_dataset(name=\"eval_dataset\",description=\"pragmatic dspy events reduced optimised\")\n",
    "for item in ds.items:\n",
    "    if item.status != DatasetStatus.ARCHIVED:\n",
    "        input = {\"html\":item.input[\"kwargs\"][\"html\"],\"url\":item.input[\"kwargs\"][\"url\"],\"title\":item.expected_output['_store']['title'],\"description\":item.expected_output['_store']['description'],\"location\":item.expected_output['_store']['location'],\"start_time\":item.expected_output['_store']['start_time'],\"end_time\":item.expected_output['_store']['end_time']}\n",
    "        pred = ground_truth_evaluator.evaluate(**input)\n",
    "        score = pred.score\n",
    "        if \"the end time\" in pred.reasoning or item.expected_output['_store']['end_time'] == \"Not Specified\" or item.expected_output['_store']['end_time'] == \"N/A\":\n",
    "            expected_output = item.expected_output\n",
    "            expected_output['_store']['end_time'] = None\n",
    "            langfuse.create_dataset_item(dataset_name=ds.name,input=item.input,expected_output=expected_output,metadata=item.metadata,id=item.id)\n",
    "            if score > 0.6:\n",
    "                score = 1\n",
    "            else:\n",
    "                score = score + 0.4\n",
    "        langfuse.create_dataset_item(dataset_name=eval_dataset.name,input=input,expected_output={\"score\":score,\"reasoning\":pred.reasoning})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll split the dataset into train and test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = langfuse.get_dataset(\"eval_dataset\")\n",
    "evaluator_examples = []\n",
    "for item in dataset.items:\n",
    "    evaluator_examples.append(dspy.Example(html=item.input[\"html\"], url=item.input[\"url\"], title=item.input[\"title\"], description=item.input[\"description\"], location=item.input[\"location\"], start_time=item.input[\"start_time\"], end_time=item.input[\"end_time\"],score=item.expected_output[\"score\"],reasoning=item.expected_output[\"reasoning\"]).with_inputs(\"html\",\"url\",\"title\",\"description\",\"location\",\"start_time\",\"end_time\"))\n",
    "evaluator_train, evaluator_test = evaluator_examples[:int(len(evaluator_examples)*0.8)], evaluator_examples[int(len(evaluator_examples)*0.8):]\n",
    "len(evaluator_train), len(evaluator_test), len(evaluator_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a metric to optimise the evaluator so lets write one that simply compares the example to the prediction\n",
    "\n",
    "We'll use that metric to optimise an new version of the evaluator and save it into gteo.json\n",
    "\n",
    "**Note**: Running compile will generate multiple LLM calls so DSPy will prompt you to confirm you're okay with the cost before running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_score(example, prediction, trace=None):\n",
    "    return 1- abs(example.score - prediction.score) \n",
    "\n",
    "from extractor import GroundTruthEvaluator_Signature\n",
    "ground_truth_evaluator = dspy.Predict(GroundTruthEvaluator_Signature)\n",
    "tp = dspy.MIPROv2(metric=same_score, auto=\"light\", num_threads=6)\n",
    "ground_truth_evaluator = tp.compile(ground_truth_evaluator, trainset=evaluator_train, max_bootstrapped_demos=2, max_labeled_demos=2)\n",
    "ground_truth_evaluator.save(\"gteo.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now running the contrived example again we can see the evaluator now prefers not to speculate an end time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_evaluator = GroundTruthEvaluator(config_file=\"gteo.json\")\n",
    "eval1 = ground_truth_evaluator.evaluate(html=test_html, url=\"https://bobsevents.com/event1\", title=\"An Event\", description=\"Great event\", location=\"Dublin\", start_time=\"2024-12-12T13:00:00\", end_time=None)\n",
    "eval2 = ground_truth_evaluator.evaluate(html=test_html, url=\"https://bobsevents.com/event1\", title=\"An Event\", description=\"Great event\", location=\"Dublin\", start_time=\"2024-12-12T13:00:00\", end_time=\"2024-12-12T15:00:00\")\n",
    "print(eval1)\n",
    "print(eval2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimising\n",
    "\n",
    "Okay to we've already optimised the evaluator but that wasn't our main goal. Lets return to optimising the extractor\n",
    "\n",
    "First we need to split the dataset into train and test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "from langfuse.api.resources.commons.types import DatasetStatus\n",
    "dataset = langfuse.get_dataset(\"events_reduced\")\n",
    "train = []\n",
    "test = []\n",
    "data_size = len(dataset.items)\n",
    "for item in dataset.items:\n",
    "    if item.status != DatasetStatus.ARCHIVED:\n",
    "        metadata = item.metadata\n",
    "        if metadata is None:\n",
    "            metadata = {}\n",
    "        if random.random() < 0.8:\n",
    "            train.append(dspy.Example(html=item.input[\"kwargs\"][\"html\"], url=item.input[\"kwargs\"][\"url\"], title=item.expected_output[\"_store\"][\"title\"], description=item.expected_output[\"_store\"][\"description\"], location=item.expected_output[\"_store\"][\"location\"], start_time=item.expected_output[\"_store\"][\"start_time\"], end_time=item.expected_output[\"_store\"][\"end_time\"]).with_inputs(\"html\",\"url\"))\n",
    "            metadata[\"set\"] = \"train\"\n",
    "        else:\n",
    "            test.append(dspy.Example(html=item.input[\"kwargs\"][\"html\"], url=item.input[\"kwargs\"][\"url\"], title=item.expected_output[\"_store\"][\"title\"], description=item.expected_output[\"_store\"][\"description\"], location=item.expected_output[\"_store\"][\"location\"], start_time=item.expected_output[\"_store\"][\"start_time\"], end_time=item.expected_output[\"_store\"][\"end_time\"]).with_inputs(\"html\",\"url\"))\n",
    "            metadata[\"set\"] = \"test\"\n",
    "        langfuse.create_dataset_item(\"events_reduced\",item.input,item.expected_output,metadata,item.source_trace_id,item.source_observation_id,item.status,item.id)\n",
    "len(train), len(test), len(dataset.items)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the evaluator as the metric so in this case we just ignore expected output from the example. It's just checking if the predicted information is grounded in the html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ogt_evaluate(example, prediction, trace=None):\n",
    "    ground_truth_evaluator = GroundTruthEvaluator(config_file=\"gteo.json\")\n",
    "    return ground_truth_evaluator.evaluate(html=example.html, url=example.url, title=prediction.title, description=prediction.description, location=prediction.location, start_time=prediction.start_time, end_time=prediction.end_time).score\n",
    "\n",
    "simple_extractor = dspy.Predict(Extractor_Signature)\n",
    "def run_optimisation():\n",
    "    tp = dspy.MIPROv2(metric=ogt_evaluate, auto=\"medium\", num_threads=3,verbose=True)\n",
    "    return tp.compile(simple_extractor, trainset=train, max_bootstrapped_demos=2, max_labeled_demos=5, requires_permission_to_run=False)\n",
    "\n",
    "optimised_extractor = run_optimisation()\n",
    "optimised_extractor.save(\"optimised_extractor.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next lets run the evaluation again with the optimised extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_evaluation(\"baseline\",dataset_name=\"events_reduced\",extractor=Extractor(),evaluator=GroundTruthEvaluator(config_file=\"gteo.json\"),set=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For my test set the optimised extractor performed a little better that the baseline.\n",
    "\n",
    "We can also try optimising using a different LLM for the prompt and task models. Let's try Claude 3.5 sonnet, it will be slower but hopefully propose better instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_extractor = dspy.Predict(Extractor_Signature)\n",
    "def run_optimisation():\n",
    "    prompt_lm = dspy.LM(model=\"bedrock/anthropic.claude-3-5-sonnet-20240620-v1:0\",temperature=0.0,aws_region_name=\"eu-central-1\",cooldown_time=30)\n",
    "    tp = dspy.MIPROv2(metric=ogt_evaluate, auto=\"medium\", num_threads=3,verbose=True, prompt_model=prompt_lm, task_model=prompt_lm,max_errors=20)\n",
    "    return tp.compile(simple_extractor, trainset=train, max_bootstrapped_demos=2, max_labeled_demos=5, requires_permission_to_run=False)\n",
    "\n",
    "optimised_extractor = run_optimisation()\n",
    "optimised_extractor.save(\"optimised_extractor_claude.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_evaluation(\"baseline_claude\",dataset_name=\"events_reduced\",extractor=Extractor(config_file=\"optimised_extractor_claude.json\"),evaluator=GroundTruthEvaluator(config_file=\"gteo.json\"),set=\"test\")\n",
    "langfuse.flush()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Again it performs a little better than the baseline and the first optimised extractor."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
